{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------- User's settings -------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Location of digested data\n",
    "input_directory = '/digested/'\n",
    "\n",
    "# Location of saved trained model\n",
    "model_directory = '/model_directory/'\n",
    "\n",
    "# Desired location for outputs\n",
    "output_directory = '/output_directory/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------- (semi)-Automatic -------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "import pickle\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "import numpy\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot\n",
    "import pandas\n",
    "import seaborn\n",
    "import sklearn.metrics\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure GPU/CPU devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------- If using Tensorflow-GPU: -------- #\n",
    "\n",
    "configuration = tensorflow.ConfigProto()\n",
    "\n",
    "configuration.gpu_options.allow_growth = True\n",
    "\n",
    "configuration.gpu_options.visible_device_list = \"0\"\n",
    "\n",
    "session = tensorflow.Session(config=configuration)\n",
    "\n",
    "keras.backend.set_session(session)\n",
    "\n",
    "\n",
    "# -------- If using Tensorflow (CPU) : -------- #\n",
    "\n",
    "# configuration = tensorflow.ConfigProto()\n",
    "\n",
    "# session = tensorflow.Session(config=configuration)\n",
    "\n",
    "# keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data queueing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_data_generator(input_x, input_y, batch_size):\n",
    "    num_examples, num_labels = input_y.shape\n",
    "    label_indices = []\n",
    "    for i in range(num_labels):\n",
    "        indices = [j for j in range(num_examples) if input_y[j,i] > 0]\n",
    "        label_indices.append(indices)\n",
    "        print(\"Label\",i,\":\",len(indices),\"examples\")\n",
    "    samples_per_label = int(batch_size / num_labels)\n",
    "\n",
    "    def generator():\n",
    "        while True:\n",
    "            x_samples = []\n",
    "            y_samples = []\n",
    "            for i in range(num_labels):\n",
    "                random.shuffle(label_indices[i])\n",
    "                indices = label_indices[i][0:samples_per_label]\n",
    "                x_samples.append( input_x[indices, ...] )\n",
    "                y_samples.append( input_y[indices, ...] )\n",
    "            x_samples = numpy.concatenate( x_samples )\n",
    "            y_samples = numpy.concatenate( y_samples )\n",
    "            batch_indices = numpy.arange(x_samples.shape[0])\n",
    "            numpy.random.shuffle(batch_indices)\n",
    "            x_samples = x_samples[batch_indices, ...]\n",
    "            y_samples = y_samples[batch_indices, ...]\n",
    "            yield (x_samples, y_samples)\n",
    "    return generator()\n",
    "\n",
    "\n",
    "def prediction_data_generator(input_x, input_y, batch_size):\n",
    "    num_examples, num_labels = input_y.shape\n",
    "    steps = int(num_examples / batch_size)\n",
    "    def generator():\n",
    "        i = 0\n",
    "        while True:\n",
    "            start = i*batch_size\n",
    "            end = (i+1)*batch_size\n",
    "            x_sample = input_x[start:end, ...]\n",
    "            y_sample = input_y[start:end, ...]\n",
    "            yield (x_sample, y_sample)\n",
    "            i = i + 1 if i < steps else 0\n",
    "    print(\"Prediction steps:\",steps)        \n",
    "    return generator(), steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function to normalize illumination discrepancy across images\n",
    "\n",
    "def min_max_norm(x, minimum=None, maximum=None):\n",
    "    channels = x.shape[-1]\n",
    "    if minimum is None and maximum is None:\n",
    "        minimum = []\n",
    "        maximum = []\n",
    "        for channel in range(channels):\n",
    "            minimum.append( x[..., channel].min() )\n",
    "            maximum.append( x[..., channel].max() )\n",
    "    result = numpy.zeros_like(x)\n",
    "    for ch in range(channels):\n",
    "        result[..., ch] = 100.0*( (numpy.ndarray.astype(x[..., ch], numpy.float32) - minimum[ch])/(maximum[ch] - minimum[ch]) )\n",
    "    return (result, minimum, maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_x = numpy.load(os.path.join(input_directory, \"training_x.npy\"))\n",
    "\n",
    "training_y = numpy.load(os.path.join(input_directory, \"training_y.npy\"))\n",
    "\n",
    "# input_directory = \"/path/to/other/input_directory/if/needed\"\n",
    "\n",
    "testing_x = numpy.load(os.path.join(input_directory, \"testing_x.npy\"))\n",
    "\n",
    "testing_y = numpy.load(os.path.join(input_directory, \"testing_y.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading training data\")\n",
    "\n",
    "# Use this function to normalize signal intensities across images\n",
    "training_x, pix_min, pix_max = min_max_norm(training_x)\n",
    "\n",
    "training_generator = training_data_generator(training_x, training_y, 32) \n",
    "\n",
    "print(training_x.shape, training_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading test data\")\n",
    "\n",
    "# Use this function to normalize signal intensities across images\n",
    "testing_x, _, _ = min_max_norm(testing_x, pix_min, pix_max)\n",
    "\n",
    "testing_generator, testing_steps = prediction_data_generator(testing_x, testing_y, 32)\n",
    "\n",
    "print(testing_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model:\n",
    "(can also load checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model( os.path.join(model_directory, 'model.h5') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(model_directory, 'model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(\n",
    "    generator=testing_generator, \n",
    "    steps=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the most crucial layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the densely/fully connected layer nearest to the classier, which is the one that has the shape of (None, number-of-classes)\n",
    "\n",
    "==================================================================\n",
    "\n",
    "Example 1: in case of classification of 7 classes, the last few layers are:\n",
    "\n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1024)              943820   \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 1024)              0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 7)                 7175      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 7)                 0         \n",
    "\n",
    "\n",
    "then look for the layer dense_1 , which has a shape of (None, 1024) \n",
    "\n",
    "==================================================================\n",
    "\n",
    "Example 2: in case of classification of 5 classes, the last few layers are:\n",
    "\n",
    "activation_49 (Activation)       (None, 8, 8, 2048)    0                                            \n",
    "_________________________________________________________________\n",
    "avg_pool (AveragePooling2D)      (None, 1, 1, 2048)    0                                            \n",
    "_________________________________________________________________\n",
    "global_average_pooling2d_1 (Glob (None, 2048)          0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)                  (None, 5)             10245    \n",
    "\n",
    "then look for the layer global_average_pooling2d_1 , which has a shape of (None, 2048) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(layers[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract_model = None # Clear cached abstract_model\n",
    "abstract_model = Sequential([layers[-4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_features = abstract_model.predict_generator(\n",
    "    generator=testing_generator,\n",
    "    steps=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Converting numeric labels into class names...')\n",
    "\n",
    "class_names = pickle.load(open(os.path.join(input_directory, \"class_names.sav\"), 'rb'))\n",
    "\n",
    "def save_metadata(file):\n",
    "    with open(file, 'w') as f:\n",
    "        for i in range(test_y.shape[0]):\n",
    "            f.write('{}\\n'.format( class_names[test_y[i]] ))     \n",
    "\n",
    "save_metadata( os.path.join(output_directory, 'metadata.tsv') )\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicted values in .TXT\n",
    "To be uploaded and viewed on http://projector.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.savetxt( os.path.join(output_directory, 'table_of_features.txt' ), extracted_features, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "Once finished, open http://projector.tensorflow.org on web-browser.\n",
    "\n",
    "Click \"Load data\" on the left panel.\n",
    "\n",
    "- Step 1: Load a TSV file of vectors >> Choose file: 'table_of_features.txt'\n",
    "\n",
    "- Step 2: Load a TSV file of metadata >> Choose file: 'metadata.tsv'\n",
    "\n",
    "Hit ESC or click outside the load data window to dismiss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicted values in .NPY\n",
    "Used for generating Tensorboard embeddings to be viewed locally on http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.save( os.path.join(output_directory, 'table_of_features.npy' ), extracted_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_features = numpy.load( 'table_of_features.npy' )\n",
    "embedding_var = tensorflow.Variable(extracted_features)\n",
    "\n",
    "embedSess = tensorflow.Session()\n",
    "\n",
    "# save variable in session\n",
    "embedSess.run(embedding_var.initializer)\n",
    "\n",
    "# save session (only used variable) to file\n",
    "saver = tensorflow.train.Saver([embedding_var])\n",
    "saver.save(embedSess, 'tf.ckpt')\n",
    "\n",
    "summary_writer = tensorflow.summary.FileWriter('./')\n",
    "\n",
    "config = tensorflow.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "embedding.metadata_path = 'metadata.tsv' # this metadata_path need to be modified later. See note.\n",
    "tensorflow.contrib.tensorboard.plugins.projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "embedSess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "Tensorboard embeddings files will be saved in the same location with this script.\n",
    "\n",
    "Collect the following files into one folder:\n",
    "\n",
    "- metadata.tsv\n",
    "- checkpoint\n",
    "- projector_config.pbtxt\n",
    "- tf.ckpt.index\n",
    "- tf.ckpt.meta\n",
    "- tf.ckpt.data-00000-of-00001\n",
    "\n",
    "Open with any text editor : \"projector_config.pbtxt\"\n",
    "\n",
    "\"/path/to/logdir/metadata.tsv\" has to be specified, CANNOT be relative path \"./metadata.tsv\", nor \"~/metadata.tsv\"\n",
    "\n",
    "Then type command in terminal: tensorboard --logdir=\"/path/to/logdir\"\n",
    "\n",
    "Next, open web-browser, connect to http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot categorical accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = pandas.read_csv(os.path.join(model_directory, 'training.csv') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.pyplot.plot(metrics[\"acc\"])\n",
    "matplotlib.pyplot.plot(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.pyplot.plot(metrics[\"loss\"])\n",
    "matplotlib.pyplot.plot(metrics[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(\n",
    "    batch_size=50,\n",
    "    x=testing_x\n",
    ")\n",
    "\n",
    "predicted = numpy.argmax(predicted, -1)\n",
    "expected = numpy.argmax(testing_y[:, :], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(expected, predicted)\n",
    "\n",
    "confusion = pandas.DataFrame(confusion)\n",
    "\n",
    "matplotlib.pyplot.figure(figsize=(12, 8))\n",
    "\n",
    "seaborn.heatmap(confusion, annot=True)\n",
    "\n",
    "matplotlib.pyplot.savefig( os.path.join(output_directory, 'confusion_matrix.eps') , format='eps', dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
